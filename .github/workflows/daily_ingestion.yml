name: Daily Crypto Data Ingestion

on:

  schedule:
    - cron: '0 3 * * *'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      mode:
        description: 'Run mode'
        required: false
        default: 'daily'
        type: choice
        options:
          - daily
          - backfill

jobs:
  ingest-data:
    runs-on: ubuntu-latest
    
    # Permissions for committing back to repository
    permissions:
      contents: write  # Required to commit and push database file
    
    # Job-level environment variables (from GitHub repository variables with fallback defaults)
    env:
      # General Configuration
      RATE_LIMIT_DELAY: ${{ vars.RATE_LIMIT_DELAY || '1.5' }}
      ENABLE_CSV_BACKUP: ${{ vars.ENABLE_CSV_BACKUP || 'true' }}
      LOG_LEVEL: ${{ vars.LOG_LEVEL || 'INFO' }}
      
      # Phase 2 Feature Flags
      ENABLE_MARKET_METRICS: ${{ vars.ENABLE_MARKET_METRICS || 'true' }}
      ENABLE_DERIVATIVES_DATA: ${{ vars.ENABLE_DERIVATIVES_DATA || 'true' }}
      
      # Phase 3 Feature Flags
      # Reddit: Enable social sentiment (will use API if available, RSS as fallback)
      ENABLE_SOCIAL_SENTIMENT: ${{ vars.ENABLE_SOCIAL_SENTIMENT || 'true' }}
      # RSS fallback: Enable if Reddit API credentials are missing (can override via vars)
      ENABLE_REDDIT_RSS: ${{ vars.ENABLE_REDDIT_RSS || (secrets.REDDIT_CLIENT_ID == '' && 'true' || 'false') }}
      # Twitter: Auto-enable if Bearer Token is configured (can override via vars)
      ENABLE_TWITTER_SENTIMENT: ${{ vars.ENABLE_TWITTER_SENTIMENT || (secrets.TWITTER_BEARER_TOKEN != '' && 'true' || 'false') }}
      # NewsAPI: Auto-enable if API key is configured (can override via vars)
      ENABLE_NEWS_SENTIMENT: ${{ vars.ENABLE_NEWS_SENTIMENT || (secrets.NEWSAPI_KEY != '' && 'true' || 'false') }}
      # Google Trends: No API key required
      ENABLE_SEARCH_TRENDS: ${{ vars.ENABLE_SEARCH_TRENDS || 'true' }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch full history to enable pushing back
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Verify database file from repository
        run: |
          if [ -f "data/market_intel.db" ]; then
            echo "‚úÖ Found existing database file in repository"
            ls -lh data/market_intel.db
            echo "   Database will be updated with new ingestion data"
          else
            echo "‚ÑπÔ∏è  No database file found in repository"
            echo "   A new database will be created during ingestion"
            # Ensure data directory exists
            mkdir -p data
          fi
      
      - name: Run data ingestion
        env:
          # Phase 1 & 2 API Keys (Required) - Secrets only
          SOSOVALUE_API_KEY: ${{ secrets.SOSOVALUE_API_KEY }}
          COINGECKO_API_KEY: ${{ secrets.COINGECKO_API_KEY }}
          
          # Phase 3 API Keys (Optional) - Secrets only
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
          NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}
          TWITTER_BEARER_TOKEN: ${{ secrets.TWITTER_BEARER_TOKEN }}
          
          # Note: Configuration and feature flags are set at job level via repository variables
          # They can be overridden here if needed, or set in GitHub repository variables UI
        run: |
          if [ "${{ github.event.inputs.mode }}" = "backfill" ]; then
            python3 main.py --backfill
          else
            python3 main.py
          fi
      
      - name: Generate summary report
        if: always()
        run: |
          echo "# Daily Ingestion Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Run Date:** $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "logs/ingestion_$(date +%Y%m%d).log" ]; then
            echo "## Log Summary" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -n 50 logs/ingestion_$(date +%Y%m%d).log >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -f "data/market_intel.db" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Database Info" >> $GITHUB_STEP_SUMMARY
            echo "Database size: $(du -h data/market_intel.db | cut -f1)" >> $GITHUB_STEP_SUMMARY
            echo "Database path: data/market_intel.db" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: Commit and push database to repository
        if: github.ref == 'refs/heads/main' && success()
        run: |
          echo "üì¶ Committing updated database to repository..."
          
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Stage database file and backups
          git add data/market_intel.db
          
          # Stage CSV backups if they exist and ENABLE_CSV_BACKUP is true
          if [ "$ENABLE_CSV_BACKUP" = "true" ] && [ -d "data/backups" ] && [ "$(ls -A data/backups 2>/dev/null)" ]; then
            git add data/backups/
            echo "   Also staging CSV backup files"
          fi
          
          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "‚ÑπÔ∏è  No changes to commit (database is up to date)"
          else
            # Show what's being committed
            echo "üìù Changes to commit:"
            git diff --staged --stat
            
            # Commit with descriptive message
            COMMIT_MSG="chore: daily data ingestion - $(date -u +%Y-%m-%d)"
            if [ "${{ github.event.inputs.mode }}" = "backfill" ]; then
              COMMIT_MSG="chore: backfill data ingestion - $(date -u +%Y-%m-%d)"
            fi
            
            git commit -m "$COMMIT_MSG"
            echo "‚úÖ Committed changes"
            
            # Push to repository
            git push
            echo "‚úÖ Pushed database to repository"
          fi
      
      # Optional: Upload artifacts as backup (not primary storage)
      - name: Upload database artifact (backup)
        if: success()
        continue-on-error: true
        uses: actions/upload-artifact@v4
        with:
          name: market-intel-db-backup
          path: data/market_intel.db
          retention-days: 7
          if-no-files-found: warn
      
      - name: Upload logs artifact
        if: always()
        continue-on-error: true
        uses: actions/upload-artifact@v4
        with:
          name: ingestion-logs-${{ github.run_number }}
          path: logs/
          retention-days: 7
          if-no-files-found: warn

